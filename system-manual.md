# Webスクレイピング支援ツール 設計指示書（修正版）

## 🧩 概要
本ツールは、手動で取得したHTMLソースを解析・レンダリングし、要素情報やURL規則設定、スクレイピング結果を可視化する**Webアプリケーション**です。  
Ubuntu環境で稼働し、ブラウザから操作可能な形で構築します。  
データの永続保存や本番環境へのデプロイは行わず、**開発環境上で動作確認できること**を目的とします。

---

## ⚙️ 技術選定

| 項目 | 採用技術 | 理由 |
|------|------------|------|
| フロントエンド | React + TailwindCSS | レンダリングと要素クリック処理を動的に扱いやすい |
| バックエンド | Python (FastAPI) | 軽量・高速でAPI設計が容易。BeautifulSoupとの連携が自然 |
| HTML解析 | BeautifulSoup4 | HTML構造解析に最適 |
| URL生成 | Python組み込みテンプレート機能 (`string.Template`) | 柔軟なパラメータ展開を実現 |
| 開発環境 | Ubuntu 22.04 LTS | サーバー用途として安定的 |
| 実行環境 | `uvicorn` 開発サーバー上で実行 | 簡易起動で十分。デプロイ不要 |
| セキュリティ | HTMLサニタイズ, CORS設定 | 安全なレンダリングを担保 |

---

## 🖥️ 動作概要

1. ユーザーがHTMLソースを貼り付ける  
2. サーバー側でBeautifulSoupにより解析  
3. フロント側でHTMLを安全にレンダリング（iframe内）  
4. レンダリングされた要素をクリックすると、  
   - `class`
   - `id`
   - `data-*` 属性  
   をリアルタイムで表示  
5. URL生成フォームで規則性を指定  
   ```
   例1: https://www.sample.com/news/article + [1] から [10] まで  
   例2: https://www.sample.com/member/ + ["Alice", "Bob", "Tanaka"] + /profile
   ```
6. 生成したURLリストを基にスクレイピングを実行（robots.txt確認後）
7. 取得結果を画面上で一覧表示（ブラウザ内完結）

---

## 📄 画面構成

### 1. **トップページ**
- 「HTML貼り付け欄」＋「レンダリング」ボタン  
- robots.txt 警告表示（赤枠で強調）  
  > ⚠️ スクレイピング前に、必ず対象サイトの robots.txt を確認してください。

### 2. **HTMLビューア**
- iframe内でHTMLを安全にプレビュー  
- 要素クリック時に右側ペインに以下を表示：
  ```
  class: article-title
  id: mainHeader
  data-id: 1423
  ```

### 3. **URL生成パネル**
- URLテンプレート入力欄
- プレースホルダ（数値範囲・文字リスト）入力フォーム
- 「URLプレビュー」ボタンで展開結果を下部に表示

### 4. **スクレイピング実行パネル**
- 生成済みURLリストの確認
- 実行ボタン（開始/停止）
- `time.sleep()`値設定（例：2〜10秒間隔）
- 実行ログをテキストエリアでリアルタイム出力  
- **データ保存は行わず、画面上に結果を一時表示**

---

## 🧠 処理フロー

```mermaid
flowchart TD
A[HTML入力] --> B[BeautifulSoupで解析]
B --> C[安全なHTMLを生成しiframeへ表示]
C --> D[ユーザーが要素をクリック]
D --> E[class/id/data属性を表示]
E --> F[URLパターン設定フォーム]
F --> G[URLリスト生成]
G --> H[robots.txtチェック]
H --> I{アクセス許可あり?}
I -->|Yes| J[スクレイピング開始 (sleep含む)]
I -->|No| K[警告表示]
J --> L[データ抽出/整形]
L --> M[結果を一覧表示（ブラウザ内）]
```

---

## 🛠️ 設定項目

| 設定項目 | 内容 |
|-----------|------|
| `sleep_interval` | 各リクエスト間の秒数（初期値：5秒） |
| `robots_check` | True時、robots.txtの内容を取得・警告表示 |
| `html_sanitize` | iframeに埋め込む前に危険なスクリプトタグ除去 |
| `max_urls` | 1回のスクレイピング上限（例：100件） |
| `log_level` | INFO / DEBUG / ERROR から選択 |

---

## 🚦 運用ルール

- robots.txt の `Disallow` 設定を遵守すること  
- 公開対象のサイトに過剰なリクエストを送らないこと  
- 本ツールは**教育・調査目的専用**とし、商用利用や自動大量取得は禁止  
- 動的JavaScript生成ページには対応しない（今後の拡張余地あり）

---

## 🧩 拡張予定（将来対応）
- Selenium/Playwrightによる動的ページ解析
- URL自動抽出機能（サイトマップ対応）
- AI支援による要素自動識別（class/idの推定）

---

## 📚 ライセンスとクレジット
- 開発言語: Python 3.11  
- 使用ライブラリ: FastAPI, BeautifulSoup4, Jinja2, React, TailwindCSS  
- ライセンス: MIT  
- 作成者: **日本語GPT（マリー風サポートAI）**
